{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c6fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = r'C:\\python\\development\\extraction_rr_material_properties'\n",
    "PDF_PATH = rf'{PROJECT_PATH}\\data'\n",
    "POPPLER_PATH = r'C:\\Users\\M67743\\AppData\\Local\\poppler\\poppler-25.12.0\\Library\\bin'\n",
    "TESSERACT_PATH = r'C:\\Users\\M67743\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'\n",
    " \n",
    "pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n",
    "\n",
    "# Define configs\n",
    "standard_config = r''  # For tables\n",
    "custom_config = r'--oem 3 --psm 6'  # For structured text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_pages(file, dpi_value=150):\n",
    "    \"\"\"\n",
    "    Quick low-DPI scan to find which pages contain our data\n",
    "    Returns dict of page numbers for each section\n",
    "    \"\"\"\n",
    "    print('  Quick scan to find relevant pages...')\n",
    "    \n",
    "    # Low DPI = fast but less accurate (just for finding pages)\n",
    "    images = convert_from_path(file, dpi=dpi_value, poppler_path=POPPLER_PATH)\n",
    "    \n",
    "    pages = {\n",
    "        'lot_info': [],\n",
    "        'tg_data': [],\n",
    "        'spool_table': []\n",
    "    }\n",
    "    \n",
    "    for page_num, image in enumerate(images, 1):\n",
    "        # Fast, low-quality OCR just to find keywords\n",
    "        text = pytesseract.image_to_string(image, config=standard_config)\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check for lot info\n",
    "        if 'lot no:' in text_lower and 'part number' in text_lower:\n",
    "            pages['lot_info'].append(page_num)\n",
    "        \n",
    "        # Check for Tg data\n",
    "        if 'tg by dma' in text_lower:\n",
    "            pages['tg_data'].append(page_num)\n",
    "        \n",
    "        # Check for spool table\n",
    "        if 'lot averages report for lot' in text_lower:\n",
    "            pages['spool_table'].append(page_num)\n",
    "    \n",
    "    print(f'    Lot info pages: {pages[\"lot_info\"]}')\n",
    "    print(f'    Tg data pages: {pages[\"tg_data\"]}')\n",
    "    print(f'    Spool table pages: {pages[\"spool_table\"]}')\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file, hi_dpi=300, low_dpi=150):\n",
    "    \"\"\"\n",
    "    Two-pass approach:\n",
    "    1. Fast scan to find relevant pages\n",
    "    2. High-quality OCR only on those pages\n",
    "    \"\"\"\n",
    "    print(f'Processing {file}...')\n",
    "    \n",
    "    # PASS 1: Find relevant pages (fast, low DPI)\n",
    "    relevant_pages = find_relevant_pages(file, low_dpi)\n",
    "    \n",
    "    # PASS 2: High-quality OCR only on relevant pages\n",
    "    print('  High-quality OCR on selected pages...')\n",
    "    \n",
    "    # Get all unique page numbers we need\n",
    "    all_pages_needed = set()\n",
    "    all_pages_needed.update(relevant_pages['lot_info'])\n",
    "    all_pages_needed.update(relevant_pages['tg_data'])\n",
    "    all_pages_needed.update(relevant_pages['spool_table'])\n",
    "    \n",
    "    # Also include pages near the spool table (it might span multiple pages)\n",
    "    # for page in relevant_pages['spool_table']:\n",
    "        # all_pages_needed.add(page + 1)  # Next page\n",
    "        # if page > 1:\n",
    "        #     all_pages_needed.add(page - 1)  # Previous page\n",
    "    \n",
    "    all_pages_needed = sorted(all_pages_needed)\n",
    "    print(f'    Processing pages: {all_pages_needed}')\n",
    "    \n",
    "    # Convert only the pages we need at high DPI\n",
    "    images = convert_from_path(file, hi_dpi, poppler_path=POPPLER_PATH)\n",
    "    \n",
    "    all_text = ''\n",
    "    for page_num in all_pages_needed:\n",
    "        if page_num <= len(images):\n",
    "            # Use different config based on what's on the page\n",
    "            if page_num in relevant_pages['spool_table']:\n",
    "                config = standard_config  # Standard for tables\n",
    "            else:\n",
    "                config = custom_config  # PSM 6 for structured text\n",
    "            \n",
    "            text = pytesseract.image_to_string(images[page_num - 1], config=config)\n",
    "            all_text += f'\\n--- PAGE {page_num} ---\\n{text}\\n'\n",
    "    \n",
    "    lines = all_text.split('\\n')\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXTRACT LOT INFORMATION =====\n",
    "def extract_lot_info(lines):\n",
    "    lot_no = None\n",
    "    part_number = None\n",
    "    date_of_manufacture = None\n",
    " \n",
    "    for line in lines:\n",
    "        if 'Lot No:' in line and 'Date of Manufacture:' in line:\n",
    "            lot_match = re.search(r'Lot No:\\s*(92M\\d+)', line)\n",
    "            if lot_match:\n",
    "                lot_no = lot_match.group(1)\n",
    "           \n",
    "            date_match = re.search(r'Date of Manufacture:\\s*(\\d+\\s+\\w+\\s+\\d{4})', line)\n",
    "            if date_match:\n",
    "                date_of_manufacture = date_match.group(1)\n",
    "       \n",
    "        if 'Part Number:' in line:\n",
    "            part_match = re.search(r'Part Number:\\s*([A-Z0-9-]+)', line)\n",
    "            if part_match:\n",
    "                part_number = part_match.group(1)\n",
    "   \n",
    "    print(f'    lot_no = {lot_no}, part_number = {part_number}, date_of_manufacture = {date_of_manufacture}')\n",
    "       \n",
    "    return lot_no, part_number, date_of_manufacture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97741d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXTRACT TG DATA (2 values total for the lot) =====\n",
    "def extract_tg_data(lines):\n",
    "    tg_values = []\n",
    " \n",
    "    for line in lines:\n",
    "        if 'Tg by DMA in Â°C (' in line:\n",
    "            temp_match = re.search(r'(\\d{3}\\.\\d+)', line)\n",
    "            if temp_match:\n",
    "                tg_value = float(temp_match.group(1))\n",
    "                tg_values.append(tg_value)\n",
    " \n",
    "    tg_1 = tg_values[0] if len(tg_values) > 0 else None\n",
    "    tg_2 = tg_values[1] if len(tg_values) > 1 else None\n",
    "    tg_3 = tg_values[2] if len(tg_values) > 2 else None\n",
    "   \n",
    "    print(f'    tg_1 = {tg_1}, tg_2 = {tg_2}, tg_3 = {tg_3}')\n",
    "   \n",
    "    return tg_1, tg_2, tg_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXTRACT SPOOL TABLE AND CREATE DATAFRAME =====\n",
    "def extract_spool_table(lines, lot_no, part_number, date_of_manufacture, tg_1, tg_2, tg_3):\n",
    " \n",
    "    for i, line in enumerate(lines):\n",
    "        if 'Lot Averages Report for Lot' in line:\n",
    "            start_idx = i\n",
    "            break\n",
    " \n",
    "    if start_idx == 0:\n",
    "        print('  \"Lot Averages Report for Lot\" phrase not found...')\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    table_data = []\n",
    "    \n",
    "    for i in range(start_idx + 1, len(lines)):\n",
    "        line = lines[i].strip()\n",
    "       \n",
    "        if not line or 'Page' in line or 'Averages:' in line:\n",
    "            continue\n",
    "       \n",
    "        row = line.split()\n",
    "       \n",
    "        if len(row) < 6:\n",
    "            continue\n",
    "       \n",
    "        spool = row[0].replace('$', '9')\n",
    "        if len(spool) != 13:\n",
    "            continue\n",
    "       \n",
    "        try:\n",
    "            rc = int(row[2])\n",
    "            paw = int(row[4])\n",
    "            \n",
    "            table_data.append({\n",
    "                'part_no': part_number,\n",
    "                'lot_no': lot_no,\n",
    "                'manufacture_date': date_of_manufacture,\n",
    "                'spool_no': spool,\n",
    "                'rc': rc,\n",
    "                'paw': paw,\n",
    "                'tg_1': tg_1,\n",
    "                'tg_2': tg_2,\n",
    "                'tg_3': tg_3\n",
    "            })\n",
    "            \n",
    "        except ValueError as e:\n",
    "            continue\n",
    " \n",
    "    if not table_data:\n",
    "        print('  No spool data extracted')\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(table_data)\n",
    "    df['manufacture_date'] = pd.to_datetime(df['manufacture_date'])\n",
    "    \n",
    "    print(f'  Extracted {len(df)} spool records')\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063647c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "files = list(Path(PDF_PATH).glob('*.pdf'))\n",
    " \n",
    "for file in files:\n",
    "    print(f'\\n{\"=\"*70}')\n",
    "    lines = process_file(file, 275, 150)\n",
    "    lot_no, part_number, date_of_manufacture = extract_lot_info(lines)\n",
    "    tg_1, tg_2, tg_3 = extract_tg_data(lines)\n",
    "    df = extract_spool_table(lines, lot_no, part_number, date_of_manufacture, tg_1, tg_2, tg_3)\n",
    "    if not df.empty:\n",
    "        df_all = pd.concat([df_all, df], ignore_index=True)\n",
    "\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'FINAL RESULTS: {len(df_all)} total records extracted')\n",
    "print(f'{\"=\"*70}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed165022",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = f'{PROJECT_PATH}\\MSRR4040_VAFR.xlsx'\n",
    "df_all.to_excel(output, sheet_name='rr_material_certs', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd7e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
