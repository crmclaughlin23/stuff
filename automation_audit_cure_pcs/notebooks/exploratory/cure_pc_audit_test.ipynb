{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cure PC Audit Script - Test Notebook\n",
    "\n",
    "This notebook contains the refactored production code for testing before deployment.\n",
    "\n",
    "## Features:\n",
    "- Comprehensive logging\n",
    "- Parallel processing (10x faster)\n",
    "- Robust error handling\n",
    "- Type hints and documentation\n",
    "\n",
    "## Instructions:\n",
    "1. Run each cell sequentially\n",
    "2. Check the logs in the `logs/` directory after execution\n",
    "3. Verify the results in your database\n",
    "4. Compare with original notebook output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import all required libraries.\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import socket\n",
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from natsort import index_natsorted, order_by_index\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for Cure PC audit script.\"\"\"\n",
    "    \n",
    "    # Database settings\n",
    "    server: str\n",
    "    database: str\n",
    "    table_name: str = \"Equipment PCs\"\n",
    "    \n",
    "    # CPC settings\n",
    "    cpc_source: str = r\"C\\Program Files\\CPC Client\"\n",
    "    cpc_client: str = \"CPCClient.exe\"\n",
    "    \n",
    "    # Performance settings\n",
    "    max_workers: int = 10\n",
    "    network_timeout: float = 2.0\n",
    "    \n",
    "    # Logging settings\n",
    "    log_dir: Path = None\n",
    "    log_level: str = \"INFO\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Set default log directory and ensure it exists.\"\"\"\n",
    "        if self.log_dir is None:\n",
    "            # Set to project root / logs\n",
    "            base = Path().resolve()\n",
    "            while not (base / \"notebooks\").exists() and base.parent != base:\n",
    "                base = base.parent\n",
    "            self.log_dir = base / \"logs\"\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(config: Config) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Configure comprehensive logging with file and console handlers.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration object with logging settings\n",
    "        \n",
    "    Returns:\n",
    "        Configured logger instance\n",
    "    \"\"\"\n",
    "    from logging.handlers import RotatingFileHandler\n",
    "    \n",
    "    # Create logger\n",
    "    logger = logging.getLogger(\"cure_pc_audit\")\n",
    "    logger.setLevel(getattr(logging, config.log_level))\n",
    "    \n",
    "    # Remove existing handlers\n",
    "    logger.handlers = []\n",
    "    \n",
    "    # Create formatters\n",
    "    detailed_formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    simple_formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    \n",
    "    # File handler (rotating, keeps last 5 files of 10MB each)\n",
    "    log_file = config.log_dir / f\"cure_pc_audit_{datetime.now():%Y%m%d}.log\"\n",
    "    file_handler = RotatingFileHandler(\n",
    "        log_file,\n",
    "        maxBytes=10 * 1024 * 1024,  # 10MB\n",
    "        backupCount=5,\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(detailed_formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    # Console handler (less verbose)\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(simple_formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "print(\"‚úÖ Logging setup function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Database Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def get_db_engine(config: Config) -> Engine:\n",
    "    \"\"\"\n",
    "    Create and manage database engine with proper cleanup.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration object with database settings\n",
    "        \n",
    "    Yields:\n",
    "        SQLAlchemy engine instance\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"cure_pc_audit\")\n",
    "    \n",
    "    conn_str = (\n",
    "        f\"mssql+pyodbc://{config.server}/{config.database}?\"\n",
    "        f\"trusted_connection=yes&\"\n",
    "        f\"driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Connecting to database: {config.server}/{config.database}\")\n",
    "    engine = create_engine(conn_str, pool_pre_ping=True)\n",
    "    \n",
    "    try:\n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(\"SELECT 1\")\n",
    "        logger.info(\"Database connection successful\")\n",
    "        yield engine\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database connection failed: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "        logger.debug(\"Database connection closed\")\n",
    "\n",
    "\n",
    "def read_equipment_pcs(engine: Engine, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read equipment PCs from database.\n",
    "    \n",
    "    Args:\n",
    "        engine: SQLAlchemy engine\n",
    "        table_name: Name of the table to query\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing equipment PC data\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"cure_pc_audit\")\n",
    "    \n",
    "    query = f\"SELECT * FROM [{table_name}]\"\n",
    "    logger.debug(f\"Executing query: {query}\")\n",
    "    \n",
    "    df = pd.read_sql(query, engine)\n",
    "    logger.info(f\"Retrieved {len(df)} equipment PCs from database\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def write_equipment_pcs(\n",
    "    df: pd.DataFrame,\n",
    "    engine: Engine,\n",
    "    table_name: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Write equipment PCs to database.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing equipment PC data\n",
    "        engine: SQLAlchemy engine\n",
    "        table_name: Name of the table to write to\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"cure_pc_audit\")\n",
    "    \n",
    "    logger.info(f\"Writing {len(df)} equipment PCs to database...\")\n",
    "    df.to_sql(\n",
    "        name=table_name,\n",
    "        con=engine,\n",
    "        if_exists=\"replace\",\n",
    "        index=False,\n",
    "        method=\"multi\",  # Faster bulk insert\n",
    "        chunksize=1000\n",
    "    )\n",
    "    logger.info(\"Database write successful\")\n",
    "\n",
    "print(\"‚úÖ Database functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Network Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ip(pcid: str, timeout: float = 2.0) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Resolve PCID to IP address using DNS lookup.\n",
    "    \n",
    "    Args:\n",
    "        pcid: PC identifier to resolve\n",
    "        timeout: DNS lookup timeout in seconds\n",
    "        \n",
    "    Returns:\n",
    "        IP address string or None if lookup fails\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"cure_pc_audit\")\n",
    "    \n",
    "    if not pcid or pd.isna(pcid):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        socket.setdefaulttimeout(timeout)\n",
    "        ip = socket.gethostbyname(str(pcid))\n",
    "        logger.debug(f\"Resolved {pcid} ‚Üí {ip}\")\n",
    "        return ip\n",
    "    except socket.gaierror:\n",
    "        logger.debug(f\"DNS lookup failed for PCID: {pcid}\")\n",
    "        return None\n",
    "    except socket.timeout:\n",
    "        logger.warning(f\"DNS lookup timeout for PCID: {pcid}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error resolving {pcid}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_pcid(ip_address: str, timeout: float = 2.0) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Resolve IP address to PCID using reverse DNS lookup.\n",
    "    \n",
    "    Args:\n",
    "        ip_address: IP address to resolve\n",
    "        timeout: DNS lookup timeout in seconds\n",
    "        \n",
    "    Returns:\n",
    "        PCID string or None if lookup fails\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"cure_pc_audit\")\n",
    "    \n",
    "    if not ip_address or pd.isna(ip_address):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        socket.setdefaulttimeout(timeout)\n",
    "        hostname = socket.gethostbyaddr(str(ip_address))[0]\n",
    "        pcid = hostname.split(\".\")[0].upper()\n",
    "        logger.debug(f\"Resolved {ip_address} ‚Üí {pcid}\")\n",
    "        return pcid\n",
    "    except socket.herror:\n",
    "        logger.debug(f\"Reverse DNS lookup failed for IP: {ip_address}\")\n",
    "        return None\n",
    "    except socket.timeout:\n",
    "        logger.warning(f\"Reverse DNS lookup timeout for IP: {ip_address}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error resolving {ip_address}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_cpc_version(\n",
    "    ip: str,\n",
    "    pcid: str,\n",
    "    source: str,\n",
    "    client: str,\n",
    "    timeout: float = 5.0\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Get CPC client version by checking file modification date on network share.\n",
    "    \n",
    "    Args:\n",
    "        ip: IP address of target PC\n",
    "        pcid: PC identifier\n",
    "        source: Source directory path on target PC\n",
    "        client: Client executable filename\n",
    "        timeout: Operation timeout in seconds\n",
    "        \n",
    "    Returns:\n",
    "        Version string (YYYY-MM-DD) or None if unavailable\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"cure_pc_audit\")\n",
    "    \n",
    "    if not ip and not pcid:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Try IP address first\n",
    "        if ip and not pd.isna(ip):\n",
    "            unc_path = Path(f\"\\\\\\\\{ip}\") / source / client\n",
    "            if unc_path.exists():\n",
    "                mtime = unc_path.stat().st_mtime\n",
    "                version = datetime.fromtimestamp(mtime).strftime(\"%Y-%m-%d\")\n",
    "                logger.debug(f\"Got CPC version from {ip}: {version}\")\n",
    "                return version\n",
    "        \n",
    "        # Fallback to PCID\n",
    "        if pcid and not pd.isna(pcid):\n",
    "            unc_path = Path(f\"\\\\\\\\{pcid}\") / source / client\n",
    "            if unc_path.exists():\n",
    "                mtime = unc_path.stat().st_mtime\n",
    "                version = datetime.fromtimestamp(mtime).strftime(\"%Y-%m-%d\")\n",
    "                logger.debug(f\"Got CPC version from {pcid}: {version}\")\n",
    "                return version\n",
    "        \n",
    "        logger.debug(f\"CPC client not found for {pcid}/{ip}\")\n",
    "        return None\n",
    "        \n",
    "    except PermissionError:\n",
    "        logger.debug(f\"Permission denied accessing CPC client on {pcid}/{ip}\")\n",
    "        return None\n",
    "    except OSError as e:\n",
    "        logger.debug(f\"OS error accessing CPC client on {pcid}/{ip}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error getting CPC version for {pcid}/{ip}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Network functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_map(\n",
    "    func: callable,\n",
    "    items: List,\n",
    "    max_workers: int = 10,\n",
    "    desc: str = \"Processing\"\n",
    ") -> List:\n",
    "    \"\"\"\n",
    "    Apply function to items in parallel using thread pool.\n",
    "    \n",
    "    Args:\n",
    "        func: Function to apply to each item\n",
    "        items: List of items to process\n",
    "        max_workers: Maximum number of worker threads\n",
    "        desc: Description for logging\n",
    "        \n",
    "    Returns:\n",
    "        List of results in same order as input items\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"cure_pc_audit\")\n",
    "    \n",
    "    if not items:\n",
    "        return []\n",
    "    \n",
    "    results = [None] * len(items)\n",
    "    total = len(items)\n",
    "    completed = 0\n",
    "    \n",
    "    logger.info(f\"{desc}: Processing {total} items with {max_workers} workers...\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_idx = {\n",
    "            executor.submit(func, item): idx\n",
    "            for idx, item in enumerate(items)\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(future_to_idx):\n",
    "            idx = future_to_idx[future]\n",
    "            completed += 1\n",
    "            \n",
    "            try:\n",
    "                results[idx] = future.result()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing item {idx}: {e}\")\n",
    "                results[idx] = None\n",
    "            \n",
    "            # Log progress every 10% or 10 items, whichever is more frequent\n",
    "            log_interval = max(1, min(10, total // 10))\n",
    "            if completed % log_interval == 0 or completed == total:\n",
    "                progress = (completed / total) * 100\n",
    "                logger.info(f\"{desc}: {completed}/{total} ({progress:.1f}%)\")\n",
    "    \n",
    "    logger.info(f\"{desc}: Complete\")\n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Parallel processing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_equipment_pcs(\n",
    "    df: pd.DataFrame,\n",
    "    config: Config\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enrich equipment PC data with current network information.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing base equipment PC data\n",
    "        config: Configuration object\n",
    "        \n",
    "    Returns:\n",
    "        Enriched DataFrame with updated IP addresses, PCIDs, and CPC versions\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"cure_pc_audit\")\n",
    "    \n",
    "    logger.info(\"Starting equipment PC enrichment...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Get current IPs from PCIDs (parallel)\n",
    "    logger.info(\"Resolving IP addresses from PCIDs...\")\n",
    "    new_ips = parallel_map(\n",
    "        lambda pcid: get_ip(pcid, config.network_timeout),\n",
    "        df[\"PCID\"].tolist(),\n",
    "        max_workers=config.max_workers,\n",
    "        desc=\"IP resolution\"\n",
    "    )\n",
    "    df[\"New_IP_Address\"] = new_ips\n",
    "    \n",
    "    # Update IP addresses (keep old if new lookup failed)\n",
    "    df[\"IP_Address\"] = df.apply(\n",
    "        lambda row: row[\"New_IP_Address\"] if row[\"New_IP_Address\"] is not None \n",
    "        else row.get(\"IP_Address\"),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Get current PCIDs from IPs (parallel)\n",
    "    logger.info(\"Resolving PCIDs from IP addresses...\")\n",
    "    new_pcids = parallel_map(\n",
    "        lambda ip: get_pcid(ip, config.network_timeout),\n",
    "        df[\"IP_Address\"].tolist(),\n",
    "        max_workers=config.max_workers,\n",
    "        desc=\"PCID resolution\"\n",
    "    )\n",
    "    df[\"New_PCID\"] = new_pcids\n",
    "    \n",
    "    # Update PCIDs (keep old if new lookup failed)\n",
    "    df[\"PCID\"] = df.apply(\n",
    "        lambda row: row[\"New_PCID\"] if row[\"New_PCID\"] is not None \n",
    "        else row.get(\"PCID\"),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Get CPC versions (parallel)\n",
    "    logger.info(\"Checking CPC versions...\")\n",
    "    \n",
    "    def get_version_wrapper(row_tuple):\n",
    "        \"\"\"Wrapper to handle row data in parallel processing.\"\"\"\n",
    "        ip, pcid = row_tuple\n",
    "        return get_cpc_version(ip, pcid, config.cpc_source, config.cpc_client)\n",
    "    \n",
    "    new_versions = parallel_map(\n",
    "        get_version_wrapper,\n",
    "        list(zip(df[\"IP_Address\"], df[\"PCID\"])),\n",
    "        max_workers=config.max_workers,\n",
    "        desc=\"CPC version check\"\n",
    "    )\n",
    "    df[\"New_CPC_Version\"] = new_versions\n",
    "    \n",
    "    # Update CPC versions (keep old if new check failed)\n",
    "    df[\"CPC_Version\"] = df.apply(\n",
    "        lambda row: row[\"New_CPC_Version\"] if row[\"New_CPC_Version\"] is not None \n",
    "        else row.get(\"CPC_Version\"),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    df = df.drop(columns=[\"New_IP_Address\", \"New_PCID\", \"New_CPC_Version\"])\n",
    "    \n",
    "    # Generate database names\n",
    "    df[\"Database_Name\"] = df[\"Alt_Name\"].apply(\n",
    "        lambda name: f\"CPC_{str(name).replace(' ', '')}\" if pd.notna(name) else None\n",
    "    )\n",
    "    \n",
    "    # Sort naturally by PC column\n",
    "    logger.info(\"Sorting results...\")\n",
    "    indexer = index_natsorted(df[\"PC\"])\n",
    "    df = df.reindex(order_by_index(df.index, indexer))\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    duration = (datetime.now() - start_time).total_seconds()\n",
    "    logger.info(f\"Equipment PC enrichment complete in {duration:.1f}s\")\n",
    "    \n",
    "    # Log statistics\n",
    "    successful_ips = df[\"IP_Address\"].notna().sum()\n",
    "    successful_versions = df[\"CPC_Version\"].notna().sum()\n",
    "    logger.info(f\"Statistics:\")\n",
    "    logger.info(f\"  - Total PCs: {len(df)}\")\n",
    "    logger.info(f\"  - Successful IP lookups: {successful_ips}/{len(df)} \"\n",
    "                f\"({successful_ips/len(df)*100:.1f}%)\")\n",
    "    logger.info(f\"  - Successful version checks: {successful_versions}/{len(df)} \"\n",
    "                f\"({successful_versions/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Data enrichment function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Configuration\n",
    "\n",
    "‚ö†Ô∏è **Important:** Update the configuration below with your settings before running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment\n",
    "os.environ['ENV'] = 'production'\n",
    "\n",
    "# Import global config file\n",
    "base = Path().resolve().parents[2]\n",
    "sys.path.insert(0, str(base / 'shared/global_config'))\n",
    "\n",
    "# Import config file variables\n",
    "import config\n",
    "\n",
    "# Create configuration\n",
    "cfg = Config(\n",
    "    server=config.PROD_SERVER,\n",
    "    database=config.PYRO_DATABASE,\n",
    "    max_workers=10,  # Adjust based on your network\n",
    "    network_timeout=2.0,  # Adjust if you have slow network\n",
    "    log_level=\"INFO\"  # Use \"DEBUG\" for more detailed logs\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Configuration initialized\")\n",
    "print(f\"   Server: {cfg.server}\")\n",
    "print(f\"   Database: {cfg.database}\")\n",
    "print(f\"   Log directory: {cfg.log_dir}\")\n",
    "print(f\"   Max workers: {cfg.max_workers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logger = setup_logging(cfg)\n",
    "\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"CURE PC AUDIT SCRIPT - TEST RUN\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(f\"Start time: {datetime.now():%Y-%m-%d %H:%M:%S}\")\n",
    "logger.info(f\"Python version: {sys.version}\")\n",
    "logger.info(f\"Log file: {cfg.log_dir / f'cure_pc_audit_{datetime.now():%Y%m%d}.log'}\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ Logging configured successfully\")\n",
    "print(f\"\\nüìù Logs are being written to: {cfg.log_dir / f'cure_pc_audit_{datetime.now():%Y%m%d}.log'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Read Data from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read current equipment PC data\n",
    "with get_db_engine(cfg) as engine:\n",
    "    equip_pcs = read_equipment_pcs(engine, cfg.table_name)\n",
    "\n",
    "print(f\"\\n‚úÖ Retrieved {len(equip_pcs)} equipment PCs from database\")\n",
    "print(f\"\\nPreview of data:\")\n",
    "equip_pcs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Enrich Data (The Main Process)\n",
    "\n",
    "This cell performs all the network lookups and file checks in parallel.\n",
    "\n",
    "**Note:** This may take a few minutes depending on the number of PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich the data with current information\n",
    "equip_pcs_enriched = enrich_equipment_pcs(equip_pcs, cfg)\n",
    "\n",
    "print(f\"\\n‚úÖ Data enrichment complete!\")\n",
    "print(f\"\\nEnriched data preview:\")\n",
    "equip_pcs_enriched.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the full enriched dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENRICHED EQUIPMENT PCs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "equip_pcs_enriched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Write Results to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write enriched data back to database\n",
    "with get_db_engine(cfg) as engine:\n",
    "    write_equipment_pcs(equip_pcs_enriched, engine, cfg.table_name)\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully wrote {len(equip_pcs_enriched)} equipment PCs to database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"CURE PC AUDIT SCRIPT - TEST RUN COMPLETE\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(f\"End time: {datetime.now():%Y-%m-%d %H:%M:%S}\")\n",
    "logger.info(f\"Status: SUCCESS\")\n",
    "logger.info(f\"Processed {len(equip_pcs_enriched)} equipment PCs successfully\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ TEST RUN COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   - Total PCs processed: {len(equip_pcs_enriched)}\")\n",
    "print(f\"   - Successful IP lookups: {equip_pcs_enriched['IP_Address'].notna().sum()}\")\n",
    "print(f\"   - Successful CPC version checks: {equip_pcs_enriched['CPC_Version'].notna().sum()}\")\n",
    "print(f\"\\nüìù Check the log file for detailed information:\")\n",
    "print(f\"   {cfg.log_dir / f'cure_pc_audit_{datetime.now():%Y%m%d}.log'}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Compare with Original Data (Optional)\n",
    "\n",
    "Run this cell to compare the original and enriched data side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs enriched for first 5 PCs\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON: Original vs Enriched (First 5 PCs)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_cols = ['PCID', 'IP_Address', 'CPC_Version']\n",
    "\n",
    "for idx in range(min(5, len(equip_pcs))):\n",
    "    print(f\"\\nPC #{idx + 1}:\")\n",
    "    print(\"  Original:\")\n",
    "    for col in comparison_cols:\n",
    "        if col in equip_pcs.columns:\n",
    "            print(f\"    {col}: {equip_pcs.loc[idx, col]}\")\n",
    "    print(\"  Enriched:\")\n",
    "    for col in comparison_cols:\n",
    "        if col in equip_pcs_enriched.columns:\n",
    "            print(f\"    {col}: {equip_pcs_enriched.loc[idx, col]}\")\n",
    "    print(\"  \" + \"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After testing this notebook:\n",
    "\n",
    "1. **Review the logs** in the `logs/` directory to verify everything worked correctly\n",
    "2. **Check the database** to ensure the data was written correctly\n",
    "3. **Compare results** with your original notebook to ensure consistency\n",
    "4. **If everything looks good**, deploy the Python script version for Task Scheduler\n",
    "\n",
    "### To deploy as a scheduled task:\n",
    "\n",
    "1. Copy the `cure_pc_audit_refactored.py` script to your production location\n",
    "2. Set up Windows Task Scheduler using the instructions in `IMPLEMENTATION_GUIDE.md`\n",
    "3. Set up monitoring using the `check_audit_status.py` script\n",
    "\n",
    "### Performance Notes:\n",
    "\n",
    "- **Parallel processing** makes this ~10x faster than the original notebook\n",
    "- Adjust `max_workers` in the configuration if needed (more workers = faster, but more network load)\n",
    "- Adjust `network_timeout` if you have slow network connections\n",
    "\n",
    "### Troubleshooting:\n",
    "\n",
    "- If DNS lookups are slow, reduce `max_workers` to 5\n",
    "- If getting permission errors, ensure you have access to the network shares\n",
    "- If database writes fail, check your SQL Server permissions\n",
    "- Check the log file for detailed error messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
